### Description
#
# How to run
# python3 validate_ycb.py <data root folder> <data name without suffix> <model path (support *.xyz and *.obj>
# ex: want to validate "./ycb_subset/data/0000/000003-color.png" data
# python3 validate_ycb.py ./ycb_subset/data/0000 000003 ./ycb_subset/models/009_gelatin_box/points.xyz
#
# this script will show an interactive plot showing three objects
# (model at origin, points cloud projected from depth image, and model applied ground truth translation and rotation)
# You should only see two points clouds becasue points cloud projected from depth image and model applied ground truth translation and rotation
# should completely overlap with each other
#
# You can further visualize it by putting three *.ply files generated by this script into MeshLab



import os
import sys
from PIL import Image
import numpy as np
import numpy.ma as ma
import scipy.io as scio
from pathlib import Path

import open3d as o3d

CUSTOM_DATASET = True

idx = 0 # ycb might have multiple in one data. Here we only validate the first object each data


### our custom dataset intrinsic matrix
if CUSTOM_DATASET:
    cam_fx = 640
    cam_fy = 640
    cam_cx = 640
    cam_cy = 360
else:
    ### ycb intrinsic matrix
    cam_cx = 312.9869
    cam_cy = 241.3109
    cam_fx = 1066.778
    cam_fy = 1067.487


def load_model_points_from_xyz(model_path):

    cld = []
    with model_path.open() as input_file:
        while 1:
            input_line = input_file.readline()
            if not input_line:
                break
            input_line = input_line[:-1].split(' ')
            cld.append([float(input_line[0]), float(input_line[1]), float(input_line[2])])
        cld = np.array(cld)
    return cld

if __name__ == '__main__':
    root = Path(sys.argv[1])
    data_index = sys.argv[2]
    model_path = Path(sys.argv[3])

    depth = np.array(Image.open(root / '{}-depth.png'.format(data_index)))
    label = np.array(Image.open(root / '{}-label.png'.format(data_index)))
    meta = scio.loadmat(root / '{}-meta.mat'.format(data_index))
    obj = meta['cls_indexes'].flatten().astype(np.int32)

    img_width = label.shape[0]
    img_length = label.shape[1]
    print("Image shape (img_width, img_length): ", img_width, img_length)

    xmap = np.array([[j for i in range(img_length)] for j in range(img_width)])
    ymap = np.array([[i for i in range(img_length)] for j in range(img_width)])


    ### calculate the projected model from depth image
    mask_depth = ma.getmaskarray(ma.masked_not_equal(depth, 0))
    mask_label = ma.getmaskarray(ma.masked_equal(label, obj[idx]))
    mask = mask_label * mask_depth

    choose = mask.flatten().nonzero()[0]
    depth_masked = depth.flatten()[choose][:, np.newaxis].astype(np.float32)
    xmap_masked = xmap.flatten()[choose][:, np.newaxis].astype(np.float32)
    ymap_masked = ymap.flatten()[choose][:, np.newaxis].astype(np.float32)

    cam_scale = meta['factor_depth'][0][0]
    pt2 = depth_masked / cam_scale
    pt0 = (ymap_masked - cam_cx) * pt2 / cam_fx
    pt1 = (xmap_masked - cam_cy) * pt2 / cam_fy
    points_projected_from_depth_image = np.concatenate((pt0, pt1, pt2), axis=1)
    print("Points projected from depth: ", points_projected_from_depth_image.shape)

    FOR = o3d.geometry.TriangleMesh.create_coordinate_frame(size=0.1, origin=[0,0,0])
    pcld_projected_from_depteh_image = o3d.geometry.PointCloud()
    pcld_projected_from_depteh_image.points = o3d.utility.Vector3dVector(points_projected_from_depth_image)


    ### read the model at origin
    if "obj" in model_path.suffix:
        model_at_origin = o3d.io.read_triangle_mesh(str(model_path))
        model_at_origin = np.array(model_at_origin.vertices)
    else:
        model_at_origin = load_model_points_from_xyz(model_path)
    print("Model at origin: ", model_at_origin.shape)

    model_at_origin_pcld = o3d.geometry.PointCloud()
    model_at_origin_pcld.points = o3d.utility.Vector3dVector(model_at_origin)


    ### apply ground truth rotation and translation on model at origin
    target_r = meta['poses'][:, :, idx][:, 0:3]
    target_t = np.array([meta['poses'][:, :, idx][:, 3:4].flatten()])
    print("Rotation shape {} | Translation shape {}".format(target_r.shape, target_t.shape))

    target = np.dot(model_at_origin, target_r.T)
    target = np.add(target, target_t)
    print("Model applied ground truth pose: ", target.shape)

    model_applied_gt_pose_pcld = o3d.geometry.PointCloud()
    model_applied_gt_pose_pcld.points = o3d.utility.Vector3dVector(target)


    ### generate visualization
    o3d.io.write_point_cloud("model_applied_gt_pose_pcld.ply", model_applied_gt_pose_pcld)
    o3d.io.write_point_cloud("pcld_projected_from_depteh_image.ply", pcld_projected_from_depteh_image)
    o3d.io.write_point_cloud("model_at_origin_pcld.ply", model_at_origin_pcld)

    o3d.visualization.draw_geometries([model_applied_gt_pose_pcld, model_at_origin_pcld, FOR, pcld_projected_from_depteh_image])




